{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "rm -r /tmp/tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def L2_dist(a, b):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(a-b)))\n",
    "\n",
    "def margin_cost(pos, neg, margin=1.):\n",
    "    out = margin + pos - neg\n",
    "    # grad is non-zero only within a certain margin\n",
    "    in_margin = tf.to_float(tf.greater(out, 0))\n",
    "    return tf.reduce_sum(out * in_margin)\n",
    "\n",
    "triples = pd.read_csv('walmart.nt', \n",
    "                      delim_whitespace=True, \n",
    "                      names=['s', 'p', 'o'])\n",
    "\n",
    "entities       = set(triples.s.values) | set(triples.o.values)\n",
    "predicates     = set(triples.p.values)\n",
    "entity_dict    = dict(enumerate(entities))\n",
    "predicate_dict = dict(enumerate(predicates))\n",
    "\n",
    "entity_ids = dict([(v,k) for k,v in entity_dict.items()])\n",
    "predicate_ids = dict([(v,k) for k,v in predicate_dict.items()])\n",
    "\n",
    "dim = 8\n",
    "bound = 6/np.sqrt(dim)\n",
    "batch_size = 1024\n",
    "\n",
    "X = np.array([triples.s.apply(lambda k: entity_ids[k]).values,\n",
    "              triples.p.apply(lambda k: predicate_ids[k]).values,\n",
    "              triples.o.apply(lambda k: entity_ids[k]).values]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('initialize_embeddings') as scope:\n",
    "    entity_embeddings    = tf.Variable(tf.random_uniform([len(entities),dim],-bound, +bound),\n",
    "                                       name='entity_embeddings')\n",
    "\n",
    "    predicate_embeddings = tf.Variable(tf.random_uniform([len(predicates),dim],-bound, +bound),\n",
    "                                       name='predicate_embeddings')\n",
    "    \n",
    "with tf.name_scope('read_inputs') as scope:\n",
    "    pos_head = tf.placeholder(tf.int32, [batch_size], name='positive_head')\n",
    "    pos_tail = tf.placeholder(tf.int32, [batch_size], name='positive_tail')\n",
    "    neg_head = tf.placeholder(tf.int32, [batch_size], name='corrupted_head')\n",
    "    neg_tail = tf.placeholder(tf.int32, [batch_size], name='corrupted_tail')\n",
    "    link     = tf.placeholder(tf.int32, [batch_size], name='link')\n",
    "\n",
    "with tf.name_scope('lookup_embeddings') as scope:\n",
    "    pos_head_vec = tf.nn.embedding_lookup(entity_embeddings, pos_head)\n",
    "    pos_tail_vec = tf.nn.embedding_lookup(entity_embeddings, pos_tail)\n",
    "    neg_head_vec = tf.nn.embedding_lookup(entity_embeddings, neg_head)\n",
    "    neg_tail_vec = tf.nn.embedding_lookup(entity_embeddings, neg_tail)\n",
    "    link_vec     = tf.nn.embedding_lookup(predicate_embeddings, link)\n",
    "    \n",
    "with tf.name_scope('normalize_embeddings') as scope:\n",
    "    pos_head_vec = tf.nn.l2_normalize(pos_head_vec, 1)\n",
    "    pos_tail_vec = tf.nn.l2_normalize(pos_tail_vec, 1)\n",
    "    neg_head_vec = tf.nn.l2_normalize(neg_head_vec, 1)\n",
    "    neg_tail_vec = tf.nn.l2_normalize(neg_tail_vec, 1)\n",
    "\n",
    "with tf.name_scope('train') as scope:\n",
    "    # compute loss for true and corrupted triple\n",
    "    pos_dist = L2_dist(tf.add(pos_head_vec, link_vec), pos_tail_vec)\n",
    "    neg_dist = L2_dist(tf.add(neg_head_vec, link_vec), neg_tail_vec)\n",
    "    diff = neg_dist - pos_dist\n",
    "    loss = margin_cost(pos_dist, neg_dist, np.sqrt(dim))\n",
    "    train = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "#with tf.name_scope('report') as scope:\n",
    "pos_hist = tf.histogram_summary('distance_true', pos_dist/batch_size)\n",
    "neg_hist = tf.histogram_summary('distance_corrupt', neg_dist/batch_size)\n",
    "diff_hist = tf.histogram_summary('distance_diff', diff/batch_size)\n",
    "loss_hist = tf.histogram_summary('loss', loss/batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.python.training.summary_io.SummaryWriter(\"/tmp/tensorflow\", sess.graph_def)\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corrupt(mat):\n",
    "    out = mat.copy()\n",
    "    new_entities = np.random.random_integers(0,len(entities)-1,len(mat))\n",
    "    mask = (np.random.rand(len(mat))+1/2).astype(np.int)\n",
    "    inv_mask = np.abs(mask-1)\n",
    "    out[:,0] *= mask\n",
    "    out[:,2] *= inv_mask\n",
    "    out[:,0] += inv_mask * new_entities\n",
    "    out[:,2] += mask * new_entities\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 took: 12 s\n",
      "Epoch 31 took: 12 s\n",
      "Epoch 32 took: 12 s\n",
      "Epoch 33 took: 12 s\n",
      "Epoch 34 took: 12 s\n",
      "Epoch 35 took: 12 s\n",
      "Epoch 36 took: 12 s\n",
      "Epoch 37 took: 12 s\n",
      "Epoch 38 took: 12 s\n",
      "Epoch 39 took: 12 s\n",
      "Epoch 40 took: 12 s\n",
      "Epoch 41 took: 12 s\n",
      "Epoch 42 took: 12 s\n",
      "Epoch 43 took: 12 s\n",
      "Epoch 44 took: 12 s\n",
      "Epoch 45 took: 12 s\n",
      "Epoch 46 took: 12 s\n",
      "Epoch 47 took: 12 s\n",
      "Epoch 48 took: 12 s\n",
      "Epoch 49 took: 12 s\n",
      "Epoch 50 took: 12 s\n",
      "Epoch 51 took: 12 s\n",
      "Epoch 52 took: 12 s\n",
      "Epoch 53 took: 12 s\n",
      "Epoch 54 took: 12 s\n",
      "Epoch 55 took: 12 s\n",
      "Epoch 56 took: 12 s\n",
      "Epoch 57 took: 12 s\n",
      "Epoch 58 took: 12 s\n",
      "Epoch 59 took: 12 s\n",
      "Epoch 60 took: 12 s\n",
      "Epoch 61 took: 12 s\n",
      "Epoch 62 took: 12 s\n",
      "Epoch 63 took: 12 s\n",
      "Epoch 64 took: 12 s\n",
      "Epoch 65 took: 12 s\n",
      "Epoch 66 took: 13 s\n",
      "Epoch 67 took: 12 s\n",
      "Epoch 68 took: 12 s\n",
      "Epoch 69 took: 12 s\n",
      "Epoch 70 took: 12 s\n",
      "Epoch 71 took: 13 s\n",
      "Epoch 72 took: 13 s\n",
      "Epoch 73 took: 12 s\n",
      "Epoch 74 took: 12 s\n",
      "Epoch 75 took: 12 s\n",
      "Epoch 76 took: 12 s\n",
      "Epoch 77 took: 13 s\n",
      "Epoch 78 took: 12 s\n",
      "Epoch 79 took: 12 s\n"
     ]
    }
   ],
   "source": [
    "n_batches = int(len(X)/batch_size)\n",
    "for epoch in range(last_epoch, last_epoch+50):\n",
    "    epoch_start = time.time()\n",
    "    #for batch_num, batch_indices in enumerate(batches):\n",
    "    X = X[np.random.permutation(X.shape[0])]\n",
    "    for i in range(n_batches):\n",
    "        sample = X[i*batch_size:(i+1)*batch_size]\n",
    "        #print(sample.shape)\n",
    "        corrupt_sample = corrupt(sample)\n",
    "        #print(corrupt_sample.shape)\n",
    "        feed  = {pos_head: sample[:,0],\n",
    "                 link    : sample[:,1],\n",
    "                 pos_tail: sample[:,2],\n",
    "                 neg_head: corrupt_sample[:,0],\n",
    "                 neg_tail: corrupt_sample[:,2]}\n",
    "        \n",
    "        sess.run(train, feed_dict=feed)\n",
    "        \n",
    "        if epoch > 0 and i % 100 == 0:\n",
    "            result = sess.run(merged, feed_dict=feed)\n",
    "            writer.add_summary(result, i*n_batches + epoch*n_batches)\n",
    "        \n",
    "        elapsed = time.time() - epoch_start\n",
    "        remaining = (n_batches - i) * (elapsed / (1.0 + i))\n",
    "        print('Batch: {:d}/{:d}, ETA: {:.0f} s'.format(i, n_batches,remaining), end='\\r')\n",
    "\n",
    "    #print('Epoch {:d} took: {:.0f} s | Loss:'.format(epoch, elapsed), loss_str)\n",
    "    print('Epoch {:d} took: {:.0f} s'.format(epoch, elapsed))\n",
    "    last_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61396319, -0.34390607,  0.36752665, ...,  0.24422398,\n",
       "         0.23334531, -0.41245151],\n",
       "       [ 0.24910051, -0.38901827, -0.54924905, ..., -0.29962975,\n",
       "        -0.25688305, -0.23794715],\n",
       "       [-0.5913465 ,  0.27450365,  0.12143519, ...,  0.04485816,\n",
       "        -0.38927367,  0.58995283],\n",
       "       ..., \n",
       "       [ 0.01336615,  0.53853577,  0.24134755, ..., -0.20717558,\n",
       "         0.18626279,  0.48572305],\n",
       "       [ 0.19225186,  0.18832542,  0.47431207, ...,  0.2303036 ,\n",
       "        -0.52475494,  0.26388609],\n",
       "       [ 0.16205913, -0.02751728,  0.06905802, ..., -0.45021504,\n",
       "         0.40441337,  0.23357819]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(pos_head_vec, feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[317751,      2,  38494],\n",
       "       [290065,      1, 290587],\n",
       "       [157825,      1, 121427],\n",
       "       ..., \n",
       "       [128788,      0, 221019],\n",
       "       [289628,      0, 142268],\n",
       "       [165740,      1, 152428]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3956563, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
